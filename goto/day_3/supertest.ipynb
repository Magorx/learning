{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"Tweets data.csv\", encoding = \"ISO-8859-1\")\n",
    "target = tweets['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_trick(s):\n",
    "    s.lower()\n",
    "\n",
    "    #n = 25\n",
    "    #arr = np.zeros(n)\n",
    "    #for i in range(len(s) - 2):\n",
    "    #    arr[hash(s[i:i+2]) % n] += 1\n",
    "    \n",
    "    n = 80\n",
    "    tarr = np.zeros(n)\n",
    "    for i in range(len(s) - 2):\n",
    "        tarr[hash(s[i:i+2]) % n] += 1\n",
    "        \n",
    "    return pd.Series(tarr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tweets.SentimentText.apply(hash_trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = target\n",
    "data.drop('target', axis=1, inplace=True)\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(data, target, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6088887110968878"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best():\n",
    "    for n in range(100, 200):\n",
    "        def hash_trick(s):\n",
    "            s.lower()\n",
    "\n",
    "            #n = 25\n",
    "            #arr = np.zeros(n)\n",
    "            #for i in range(len(s) - 2):\n",
    "            #    arr[hash(s[i:i+2]) % n] += 1\n",
    "            \n",
    "            tarr = np.zeros(n)\n",
    "            for i in range(len(s) - 2):\n",
    "                tarr[hash(s[i:i+2]) % n] += 1\n",
    "\n",
    "            return pd.Series(tarr)\n",
    "\n",
    "        data = tweets.SentimentText.apply(hash_trick)\n",
    "        X_train, X_test, Y_train, Y_test = model_selection.train_test_split(data, target, test_size=0.25, random_state=42)\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, Y_train)\n",
    "        print(n, ') ', roc_auc_score(Y_test, model.predict(X_test)), sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100) 0.6236135425006899\n",
      "101) 0.6114558406878157\n",
      "102) 0.6285765929182857\n",
      "103) 0.6148839115140056\n",
      "104) 0.6093571179778077\n",
      "105) 0.6134148998569061\n",
      "106) 0.6161408833822627\n",
      "107) 0.6223431885657591\n",
      "108) 0.6165069111777576\n",
      "109) 0.6181202983710821\n",
      "110) 0.6126383630302126\n",
      "111) 0.6230886262234224\n",
      "112) 0.607474204652888\n",
      "113) 0.6203453025553338\n",
      "114) 0.6287319002836244\n",
      "115) 0.6224592921301386\n",
      "116) 0.6279517823404971\n",
      "117) 0.6251309934695515\n",
      "118) 0.6203130725074298\n",
      "119) 0.6240538313422326\n",
      "120) 0.6230867414252994\n",
      "121) 0.6296266139526328\n",
      "122) 0.6202763189440305\n",
      "123) 0.6288078576479831\n",
      "124) 0.6261510462137421\n",
      "125) 0.6276078066830417\n",
      "126) 0.6298201827198693\n",
      "127) 0.6311167353487103\n",
      "128) 0.6268288196187882\n",
      "129) 0.6209931076702238\n",
      "130) 0.6197233191747298\n",
      "131) 0.6255198273223351\n",
      "132) 0.6158689070131076\n",
      "133) 0.6306575985259371\n",
      "134) 0.6337565835998438\n",
      "135) 0.6209218623011727\n",
      "136) 0.6247589343200628\n",
      "137) 0.6360115560742521\n",
      "138) 0.6287592298564086\n",
      "139) 0.6310349351101703\n",
      "140) 0.6235651031889277\n",
      "141) 0.6270877908808943\n",
      "142) 0.627429504780602\n",
      "143) 0.6354498862335853\n",
      "144) 0.6329468743261846\n",
      "145) 0.635257071385598\n",
      "146) 0.6339889792084149\n",
      "147) 0.6406279921170203\n",
      "148) 0.6266331775736165\n",
      "149) 0.6322660852441417\n",
      "150) 0.6367831924258256\n",
      "151) 0.6224419519874065\n",
      "152) 0.6365698332782972\n",
      "153) 0.6376944923183168\n",
      "154) 0.629130535086648\n",
      "155) 0.6379415893522477\n",
      "156) 0.6182509148810089\n",
      "157) 0.6306839856996597\n",
      "158) 0.6347939764867664\n",
      "159) 0.6344773304020952\n",
      "160) 0.6325457892856012\n",
      "161) 0.6333407971339006\n",
      "162) 0.6386510273657608\n",
      "163) 0.6325423966489797\n",
      "164) 0.6380311172630921\n",
      "165) 0.6456492827966183\n",
      "166) 0.6375314572806736\n",
      "167) 0.6359173161680999\n",
      "168) 0.6217553200311821\n",
      "169) 0.6170440786428247\n",
      "170) 0.6379251916085772\n",
      "171) 0.6432834841925751\n",
      "172) 0.6339985916788424\n",
      "173) 0.6306336615897744\n",
      "174) 0.6398916014903475\n",
      "175) 0.6318755550730473\n",
      "176) 0.6315302600569058\n",
      "177) 0.6332657821686034\n",
      "178) 0.6319072196815143\n",
      "179) 0.6341015016563606\n",
      "180) 0.6390662483922671\n",
      "181) 0.6457604858858778\n",
      "182) 0.6366812248473691\n",
      "183) 0.6425152404776229\n",
      "184) 0.6297792826005991\n",
      "185) 0.6428437607904692\n",
      "186) 0.6333773622174876\n",
      "187) 0.6416184535306791\n",
      "188) 0.6455367603486726\n",
      "189) 0.643846473391928\n",
      "190) 0.6467108010995158\n",
      "191) 0.6463274331612889\n",
      "192) 0.6413634403446316\n",
      "193) 0.6346043657955884\n",
      "194) 0.6404172716868641\n",
      "195) 0.6361027803034073\n",
      "196) 0.643206584429155\n",
      "197) 0.6350007388408643\n",
      "198) 0.6449042220985794\n",
      "199) 0.6453744792302787\n"
     ]
    }
   ],
   "source": [
    "find_best()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
